
---
title: "Variograms"
output: rmarkdown::pdf_document
date: "`r format(Sys.time(), '%Y %B %d')`"
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  warning=FALSE, 
  message=FALSE,  
  comment = "#>"
)
```

# Required library
```{r library}
library(forecast)
# This is needed for
# forecast::auto.arima(y,d,seasonal,stationary)
# See Section 8.1 to 8.7 of H&A textbook
```


# Functions to predict up to n.ahead, and get lower limit, point forecast, upper limit

```{r customized-functions}
#' @description
#' Get 1 to n.ahead step forecasts at end of training set for dlny ( diff(log(y)) )
#' @param armaobj output object from arima or auto.arima
#' @param n.ahead number of steps ahead, for predict() function
#' @param y_ntrain last y value in training set
#' @param se.fit logical value for predict()
#' @param alpha  for central 100*(1-alpha)% prediction intervals;
#'     alpha=0.10 for 90% intervals
#' @return prediction interval for 1-step ahead forecast; 
#'    vectors of point forecasts and corresponding SEs.
#' @details   1 to n.ahead step forecasts are printed for dlny, not y
dlny_predict = function(armaobj, y_ntrain, n.ahead, se.fit, alpha)
{ predobj = predict(armaobj, n.ahead=n.ahead, se.fit=se.fit)
  cv = qnorm(1-alpha/2)
  lwr = predobj$pred - cv*predobj$se
  upr = predobj$pred + cv*predobj$se
  out = cbind(lwr, predobj$pred ,upr)
  colnames(out) = c("lwr","pred","upr")
  print(out)
  # prediction interval for Y_{ntrain+1} : 1-step forecast
  ypred1step = exp(out[1,]) * y_ntrain
  cat("1-step ahead forecast interval for y\n")
  print(round(ypred1step,3))
  list(ypred1step=ypred1step, pred=predobj$pred, se=predobj$se)
}

#' @description
#' Get 1 to n.ahead step forecasts at end of training set for dy  ( diff(y) )
#' @param armaobj output object from arima or auto.arima
#' @param n.ahead number of steps ahead, for predict() function
#' @param y_ntrain last y value in training set
#' @param se.fit logical value for predict()
#' @param alpha  for central 100*(1-alpha)% prediction intervals;
#'     alpha=0.10 for 90% intervals
#' @return prediction interval for 1-step ahead forecast; 
#'    vectors of point forecasts and corresponding SEs.
#' @details   1 to n.ahead step forecasts are printed for dy, not y
dy_predict = function(armaobj, y_ntrain, n.ahead, se.fit, alpha)
{ predobj = predict(armaobj, n.ahead=n.ahead, se.fit=se.fit)
  cv = qnorm(1-alpha/2)
  lwr = predobj$pred - cv*predobj$se
  upr = predobj$pred + cv*predobj$se
  out = cbind(lwr, predobj$pred ,upr)
  colnames(out) = c("lwr","pred","upr")
  print(out)
  # prediction interval for Y_{ntrain+1} : 1-step forecast
  ypred1step = out[1,]+y_ntrain
  cat("1-step ahead forecast interval for y\n")
  print(round(ypred1step,3))
  list(ypred1step=ypred1step, pred=predobj$pred, se=predobj$se)
}

#' @description
#' Get 1 to n.ahead step forecasts at end of training set for y 
#' @param armaobj output object from arima or auto.arima
#' @param n.ahead number of steps ahead, for predict() function
#' @param se.fit logical value for predict()
#' @param alpha  for central 100*(1-alpha)% prediction intervals;
#'     alpha=0.10 for 90% intervals
#' @return prediction interval for 1-step ahead forecast; 
#'    vectors of point forecasts and corresponding SEs.
#' @details   1 to n.ahead step forecasts are printed for y
y_predict = function(armaobj, n.ahead, se.fit, alpha)
{ predobj = predict(armaobj, n.ahead=n.ahead, se.fit=se.fit)
  cv = qnorm(1-alpha/2)
  lwr = predobj$pred - cv*predobj$se
  upr = predobj$pred + cv*predobj$se
  out = cbind(lwr, predobj$pred ,upr)
  colnames(out) = c("lwr","pred","upr")
  print(out)
  # prediction interval for Y_{ntrain+1} : 1-step forecast
  ypred1step = out[1,]
  cat("1-step ahead forecast interval for y\n")
  print(round(ypred1step,3))
  list(ypred1step=ypred1step, pred=predobj$pred, se=predobj$se)
}
```


# Set up dataframe after some wrangling and create some visualization plots
``` {r dataframe, fig.height=5, fig.width=5}
df = read.csv("gdp-unemploy-FRED.csv", header=T, nrow=236)
# Rename some variable, create other variables from unemployment 

# Generic y variable is used in order that code can be used for other variables.
y = df$unempl
dy = c(NA,diff(y))
dlny = df$diflnunempl/100  
# Change from 100[log(y_t)-log_{y-1}] to [log(y_t)-log_{y-1}] for simpler
# interpretation later

unempl_df = data.frame(y=y,dy=dy,dlny=dlny) 
# restrict to start in 1987 rather than 1961, omit 104 rows
unempl_df = unempl_df[-(1:104),]  # now 1987-01-01 to 2019-10-01 by quarter

# Create ts object
unempl_ts = ts(unempl_df, start=c(1987,1),end=c(2019,4),frequency=4)
plot(unempl_ts)

par(mfrow=c(2,2))
acf(unempl_ts[,'dy']);  pacf(unempl_ts[,'dy'])
acf(unempl_ts[,'dlny']);  pacf(unempl_ts[,'dlny'])

# pacf for partial autocorrelation function for conditional dependence .
# To be explained later.

ntotal = nrow(unempl_ts)

ntrain = 80
ytrain = window(unempl_ts,start=c(1987,1),end=c(2006,4))
ntrain = nrow(ytrain)

summary(ytrain)
```

# Time series model for difference of consecutive log(unemployment rate)
``` {r dlny, fig.height=5, fig.width=5}

# auto.arima tries to fit several time series models and returns a model
# that minimizes AIC = Akaike information criterion
# log-likelihood and AIC to be explained later.
# d= 0 for no differencing, stationary=T to find "best" ARMA model
dlny_auto = forecast::auto.arima(ytrain[,'dlny'],seasonal=F,stationary=T)
print(dlny_auto)

# Check if stats::arima() has same results
# AR(2) for dlny with maximum likelihood (ML) estimation
dlny_ar2ml = arima(ytrain[,'dlny'],order=c(2,0,0),method="ML")
print(dlny_ar2ml)

# AR(2) for dlny with  conditional sum of squares (CSS) estimation
dlny_ar2css = arima(ytrain[,'dlny'],order=c(2,0,0),method="CSS")
print(dlny_ar2css)

# AR(2) for dlny with using least squares and stats::lm()
# AR2 lm with dlny
dlny = ytrain[,'dlny']
dlny_lm = lm(dlny[-c(1,2)] ~ dlny[-c(1,ntrain)]+ dlny[-c(ntrain-1,ntrain)])
print(summary(dlny_lm))

y_ntrain = unempl_df$y[ntrain]
alpha = 0.10

pred1 = dlny_predict(dlny_auto, y_ntrain, n.ahead=4, se.fit=T, alpha)
pred2 = dlny_predict(dlny_ar2ml, y_ntrain, n.ahead=4, se.fit=T, alpha)
pred3 = dlny_predict(dlny_ar2css, y_ntrain, n.ahead=4, se.fit=T, alpha)

# Compare parametrizations 
b_css = dlny_ar2css$coef
print(b_css)  # from arima()
phi1 = b_css[1]; phi2 = b_css[2]; mu = b_css[3] 
b0_reg = mu*(1-phi1-phi2)
print(b0_reg) # this is same as intercept from lm
print(dlny_lm$coef) # from lm(); estimated slopes match
```

Comparing lm() and arima when model is AR;
this is illustrated for AR(2), but the pattern works for AR(p).
The prediction equation for lm() is
  $${\hat\beta}_0 + {\hat\beta}_1 * dlny_{t-1} + {\hat\beta}_2 * dlny_{t-2}$$
The prediction equation for arima( order=c(2,0,0)) is
  $${\hat\mu} + {\hat\phi}_1 * (dlny_{t-1}-{\hat\mu}) + 
  {\hat\phi}_2 * (dlny_{t-2}-{\hat\mu}) $$
Hence
  $${\hat\beta}_1 = {\hat\phi}_1$$
  $${\hat\beta}_2 = {\hat\phi}_2$$
  $${\hat\beta}_0 = {\hat\mu}(1-{\hat\phi}_1-{\hat\phi}_2)$$
This explains the above match of output for arima() and lm().


# Time series model for difference of consecutive unemployment rate
``` {r dy, fig.height=5, fig.width=5}
# Response variable is dy ; ARIMA(p,0,q) is used
# Response variable is now dy = diff(y) and not diff(log(y))
dy_auto = auto.arima(ytrain[,'dy'],seasonal=F,stationary=T)
print(dy_auto)

y_ntrain = unempl_df$y[ntrain]
alpha = 0.10



pred4 = dy_predict(dy_auto, y_ntrain, n.ahead=4, se.fit=T, alpha)

# AR(1) for dy with maximum likelihood (ML) estimation, compare auto.arima
dy_ar1 = arima(ytrain[,'dy'],order=c(1,0,0), method="ML")
print(dy_ar1)
pred5 = dy_predict(dy_ar1, y_ntrain, n.ahead=4, se.fit=T, alpha)
```

# Time series model for unemployment rate
``` {r y, fig.height=5, fig.width=5}
# Response variable is y ; ARIMA(p,1,q) is used
y_auto = auto.arima(ytrain[,'y'],d=1,seasonal=F,stationary=F)
print(y_auto)

# ARIMA(3,1,1) for y with maximum likelihood (ML) estimation, compare auto.arima
y_arima311 = arima(ytrain[,'y'],order=c(3,1,1))
print(y_arima311)

# Note the inconsistency in auto.arima with the original and differenced series.
# This is likely because several different models have similar AIC values.

# Note that auto.arima and arima agree for parameter estimates, but have
# small differences (second decimal place) for forecast intervals

# Up to 4-step ahead forecast at end of training set
alpha = 0.10
pred6 = y_predict(y_auto, n.ahead=4, se.fit=T, alpha)

pred7 = y_predict(y_arima311, n.ahead=4, se.fit=T, alpha)


# ARIMA(1,1,0) for y with maximum likelihood (ML) estimation, 
# compare ARMA(1,0,0) for dy = differenced series
# A difference is due to no intercept term (mu=0) for ARIMA;
#    the AR1 coefficent and sigma^2 estimates are qualitatively similar.
y_arima110 = arima(ytrain[,'y'],order=c(1,1,0))
print(y_arima110)

```

# 1-step forecast interval at end of training set
``` {r 1stepahead, fig.height=5, fig.width=5}
oneahead = rbind(pred1$ypred1step,pred2$ypred1step,pred3$ypred1step,
   pred4$ypred1step,pred5$ypred1step,pred6$ypred1step,pred7$ypred1step)
# 7 comparisons: auto.arima, arima(ML), arima(CSS) for dlny;
# auto.arima, arima(ML) for dy;      auto.arima, arima(ML) for y.
rownames(oneahead) = c("dlny1", "dlny2", "dlny3", "dy1", "dy2", "y1","y2")
print(oneahead)

actual = unempl_df$y[ntrain+1]
print(actual)
```

# Many-step ahead point forecasts at end of training set
``` {r morestepsahead, fig.height=5, fig.width=5}
# Get 1-step to 4-step ahead forecasts
# from dlny model with pred2, dy2 model with pred5 and y model with pred7
y_ntrain = unempl_df$y[ntrain]
fc7 = pred7$pred

fc5 = pred5$pred
fc5 = y_ntrain + c( fc5[1], sum(fc5[1:2]), sum(fc5[1:3]), sum(fc5[1:4]) )

fc2 = pred2$pred
fc2 = y_ntrain * c( exp(fc2[1]), exp(sum(fc2[1:2])), 
  exp(sum(fc2[1:3])), exp(sum(fc2[1:4])) ) 

print(fc5)
print(fc2)
print(fc7)
```

Explanations for the point forecasts. Let $n=n_{train}$ be size of training set.
Models with dy use variable $y'$ for consecutive differences.
So 
 $${\hat{y}}_{n+2} = {\hat{y}}_{n+2|n} = y_n + {\hat{y'}}_{n+1}
  + {\hat{y'}}_{n+2}$$
 $${\hat{y}}_{n+3} = {\hat{y}}_{n+2|n} = y_n + {\hat{y'}}_{n+1}
  + {\hat{y'}}_{n+2} + {\hat{y'}}_{n+3}$$
Models with dlny use variable $\ell'$ for consecutive log differences or log
of ratio of consecutive values.
$\exp\{{\hat{\ell'}}_{n+h}\}$ estimates a ratio of consecutive values.
 $${\hat{y}}_{n+1} = {\hat{y}}_{n+1|n} = y_n * \exp\{{\hat{\ell'}}_{n+1}\}$$
 $${\hat{y}}_{n+2} = {\hat{y}}_{n+2|n} = y_n * 
   \exp\{{\hat{\ell'}}_{n+1}\} * \exp\{{\hat{\ell'}}_{n+2}\} = 
  y_n *  \exp\{{\hat{\ell'}}_{n+1} + {\hat{\ell'}}_{n+2}\}$$
 $${\hat{y}}_{n+3} = {\hat{y}}_{n+3|n} = y_n * 
   \exp\{{\hat{\ell'}}_{n+1} + {\hat{\ell'}}_{n+2} + {\hat{\ell'}}_{n+2}\}$$

# Future topics
``` {r future, fig.height=5, fig.width=5}
# 
# To get 2-step ahead prediction intervals for y after fitting model for
# dy or dlny is non-trivial 
# (need more theoretical calculations based on the theory).

# To be presented in future lectures and data examples.

# For ML, the likelihood function assumes epsilon's are iid N(0,sigma^2).
# For CSS, the assumption is that E(epsilon)=0, Var(epsilon)=sigma^2.
# Derivations of SEs for prediction intervals.
# Explanations for all outputs of arima().
# Moving 1-step forecasts for holdout set.

# Diagnostics for fitted model, e.g., compare sample acf versus model-based acf
# for (differenced) series, look at out-of-sample predictions.
# General statistical modelling: best model based on likelihood AIC need not 
# be an adequate model if the classes of models being considered are too narrow.

# Advantage of differencing after log: 
#   forecast intervals cannot have lower limit below 0?
```
